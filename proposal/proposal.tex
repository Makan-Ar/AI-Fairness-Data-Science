\documentclass[11pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{url}
%\usepackage{natbib}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}


\begin{document}

\title{Analyzing Fairness of Algorithms}
\author{Anonymized Submission}
\date{\today}
\maketitle

\section{Abstract}
As more machine learning applications are being trusted with making decisions that directly or indirectly affect human lives, the potential lack of fairness or discriminatory decisions have become a growing concern within the machine learning community.

\section{Introduction}
Machine learning is no longer an esoteric subject. Over the past decade, due to its ease of implementation and reliable high accuracy, it has been adopted in a verity of diverse fields such as finance  \cite{huang2007credit, tsai2008using, galindo2000credit}, crime prediction \cite{brennan2009evaluating} and so on. Decision making in these areas has legal, moral and ethical implications all of which should be considered while aiming for increasing prediction accuracy.

Interest in non-discriminatory AI has been increasing not only due to moral reasons, but also due to numerous anti-discriminatory laws in many countries. Discrimination is the prejudicial treatment of an individual based on membership in a legally protected group such as a race or gender \cite{calmon2017optimized}. These laws typically evaluate the fairness of a decision making process by means of two distinct notions: \textbf{disparate treatment} and \textbf{disparate impact} \cite{zafar2017fairness}. Disparate treatment occurs when protected attributes are used explicitly in making decisions, also known as direct discrimination. More pervasive nowadays is disparate impact, in which protected attributes are not used but reliance on variables correlated with them leads to significantly different outcomes for different groups. This can also be labled as indirect discrimination. Indirect discrimination may be intentional, as in the historical practice of \textit{redlining} in the U.S. in which home mortgages were denied in zip codes populated primarily by minorities. However, the doctrine of disparate impact applies regardless of actual intent \cite{calmon2017optimized}.

\section{Background}


\section{Related Work}
Exploiting fairness in AI has been gaining attention. There are 

\section{Proposed Activities}

\section{Expected outcomes and Deliverables}
 
\bibliographystyle{ieeetr}
\bibliography{proposal} 

\end{document}